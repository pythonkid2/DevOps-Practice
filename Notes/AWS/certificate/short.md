
## Amazon Guard duty 

**GuardDuty = Continuous Threat Detection**

* Monitors **AWS accounts, workloads, S3 data**
* Uses **CloudTrail, VPC Flow Logs, DNS Logs**
* Adds **threat intel, anomaly detection, ML**
* Finds **malicious activity & risks** automatically

**G-CANDy** üç¨ (think of it as ‚ÄúGuardDuty gives you Candy = Security Sweetness‚Äù)

**G-CANDy stands for:**

* **G** ‚Äì Guard your **Accounts, Workloads, S3**
* **C** ‚Äì **CloudTrail** events
* **A** ‚Äì **Anomaly detection** (ML-powered)
* **N** ‚Äì **Network logs** (VPC Flow Logs)
* **D** ‚Äì **DNS logs**
* **y** ‚Äì Threat **intel (malicious IPs)**


### üïµÔ∏è **Macie** ‚Üí **M-CLASS** üéì

Focus: **Data Security for S3** (PII, sensitive data)

**M-CLASS:**

* **M** ‚Äì Macie
* **C** ‚Äì **Classifies** data (detects PII)
* **L** ‚Äì **Logs** & alerts for findings
* **A** ‚Äì **Analyzes** S3 buckets
* **S** ‚Äì **Sensitive data protection**
* **S** ‚Äì **Security compliance** (GDPR, HIPAA)

---

### üß™ **Inspector** ‚Üí **I-CVSS** üîç

Focus: **Automated vulnerability management**

**I-CVSS:**

* **I** ‚Äì Inspector
* **C** ‚Äì **Continuous scanning** of EC2, ECR, Lambda
* **V** ‚Äì **Vulnerability assessment**
* **S** ‚Äì **Security findings** with CVSS scores
* **S** ‚Äì **Seamless integration** (auto remediation with Security Hub)



Here‚Äôs a **crisp, exam-friendly note for Amazon RDS Custom** (perfect for quick revision):

---

## üìå **Amazon RDS Custom ‚Äì Exam Notes**

### 1Ô∏è‚É£ **What (Definition)**

Amazon **RDS Custom** = Managed database service for applications that require **custom OS/DB configuration** that standard RDS does not allow.
Think of it as: **"RDS + Root Access"** (but still managed by AWS).

---

### 2Ô∏è‚É£ **Why (Use Cases)**

* **Legacy/Custom apps** needing special OS configs, agents, or database patches.
* **Vendor apps** (ERP, CRM) that require custom DB settings.
* Need **SSH access** to database host for troubleshooting or installing software.
* Need to control DB and OS patching schedule but still want automated backups, monitoring.

---

### 3Ô∏è‚É£ **How (Configuration)**

* Choose **RDS Custom for Oracle or SQL Server** (supported engines as of now).
* AWS provisions an EC2 instance under the hood with DB pre-installed.
* You get **SSH access** + **OS-level customization** ability.
* Can still use:

  * Automated backups
  * Monitoring (CloudWatch)
  * Read replicas (where supported)
* Responsibility split:

  * **AWS**: infrastructure, automated backups, storage, failover.
  * **You**: OS-level changes, DB patching, security hardening.

---

### 4Ô∏è‚É£ **Common Pitfalls / Limits**

* More expensive than normal RDS (since you manage more control).
* Misconfigurations can break automation (backups, monitoring) ‚Äì you are responsible.
* No full hands-off like RDS Standard.
* Supported only for **Oracle & SQL Server** (as of exam).
* Root access means you must be careful with changes ‚Äî AWS automation can pause if instance is misconfigured.

---

### 5Ô∏è‚É£ **Exam Tips / Keywords**

* **"Need OS-level or DB-level customization" ‚Üí RDS Custom**
* **"Need SSH access to DB host" ‚Üí RDS Custom**
* **"Legacy apps requiring special agents / patches" ‚Üí RDS Custom**
* **"Fully managed, no customization" ‚Üí RDS (Standard)**

Think of it as **"Managed EC2 DB"** vs. "Fully-managed RDS".

---

Here‚Äôs a **crisp, exam-friendly note** for **Migrating RDS MySQL ‚Üí Aurora MySQL + Aurora Replicas + Auto Scaling** (perfect for quick revision):

---

## üìå **Amazon Aurora Migration & Replica Notes (Exam Focus)**

### 1Ô∏è‚É£ **Migration: RDS MySQL ‚Üí Aurora MySQL**

* **Use AWS DMS (Database Migration Service)** or **Snapshot Restore**:

  * Take snapshot of RDS MySQL ‚Üí Restore snapshot as Aurora MySQL cluster.
  * Minimal downtime migration (DMS for near-zero downtime).
* After migration, **update application endpoints** to point to Aurora cluster writer endpoint.

---

### 2Ô∏è‚É£ **Aurora Architecture & Benefits**

* **Distributed, fault-tolerant storage** (decoupled from compute).
* **Auto-scales storage** up to **128 TiB** per database.
* Data replicated across **3 AZs** (6 copies total).
* **Continuous backup to S3**, point-in-time recovery.
* **Faster failover** (30 seconds or less typically).
* **Higher throughput** vs MySQL (up to 5x faster).

---

### 3Ô∏è‚É£ **Aurora Replicas vs MySQL Read Replicas**

| Feature          | MySQL Read Replica          | Aurora Replica               |
| ---------------- | --------------------------- | ---------------------------- |
| Replication Type | Asynchronous                | Synchronous (shared storage) |
| Replica Lag      | Seconds (sometimes minutes) | \~10s of milliseconds        |
| Max Replicas     | 5                           | 15                           |
| Failover         | Manual or semi-auto         | Automated failover possible  |
| Storage          | Separate copy               | Shared storage volume        |

‚úÖ **Exam Keyword:** "Need <1 second replication lag / near real-time reads" ‚Üí **Aurora Replicas**

---

### 4Ô∏è‚É£ **Aurora Auto Scaling**

* **Aurora Replicas** can scale up/down automatically based on:

  * **CPU utilization**
  * **Connections**
  * **Average replica lag**
* **Aurora Auto Scaling = Add/Remove replicas automatically**
* Helps handle sudden read workload spikes cost-effectively.

---

### 5Ô∏è‚É£ **Exam Tips / Keywords**

* **"Minimal replica lag"** ‚Üí Aurora Replica (shared storage).
* **"Auto scale read replicas"** ‚Üí Aurora Auto Scaling.
* **"Cross-AZ replication + continuous backup + 15 replicas"** ‚Üí Aurora.
* **"Migrate MySQL to Aurora with minimal downtime"** ‚Üí DMS or snapshot restore.

---

### üß† **Memory Hook (For Exam)**

**AURORA = A-U-R-O-R-A**

* **A**uto-scaling
* **U**ltra-fast replication (ms lag)
* **R**eplicas up to 15
* **O**ptimized storage (128 TiB, 3 AZs)
* **R**ecovery (PITR + S3 backup)
* **A**vailability (HA, self-healing)

---
<img width="1024" height="1536" alt="image" src="https://github.com/user-attachments/assets/31091f5f-2332-4c5a-aa29-c51bfbd6fd3e" />



Here‚Äôs a **crisp, exam-focused note for AWS DataSync** for quick revision:

---

## üìå **AWS DataSync ‚Äì Exam Notes**

### 1Ô∏è‚É£ **What (Definition)**

AWS **DataSync** = **Online data transfer service** for moving large amounts of data quickly **to, from, and between** AWS storage services.

* Fully managed
* 10x faster than open-source tools (rsync)
* Secure, automated, scalable

---

### 2Ô∏è‚É£ **Why (Use Cases)**

* **Migrate on-premises data ‚Üí AWS** (S3, EFS, FSx)
* **Replicate data between AWS storage services** (S3 <-> EFS, FSx <-> FSx)
* **Disaster recovery**: Continuous sync between on-prem and AWS
* **Big data processing**: Quickly load datasets into AWS

---

### 3Ô∏è‚É£ **How (Configuration)**

* Deploy a **DataSync Agent** on-premises (VMware, Hyper-V, KVM).
* Define **Source Location** (on-prem NFS/SMB, AWS service).
* Define **Destination Location** (S3, EFS, FSx, etc.).
* Create **Tasks** ‚Üí schedule for one-time or recurring transfers.
* Supports **encryption (TLS), data integrity checks, and IAM** for permissions.

---

### 4Ô∏è‚É£ **Common Pitfalls / Limits**

* **Agent required** for on-premises sources (not needed for cloud-to-cloud).
* Transfers only **file data** (not block storage snapshots).
* Pricing based on **data transferred (per GB)**.

---

### 5Ô∏è‚É£ **Exam Tips / Keywords**

* **"Fast, secure, automated online data transfer" ‚Üí DataSync**
* **"10x faster than open-source" ‚Üí DataSync**
* **"Agent for on-prem" ‚Üí DataSync**
* **"Move data between S3, EFS, FSx" ‚Üí DataSync**
* **"Recurring scheduled sync" ‚Üí DataSync**

---

### üß† **Memory Hook**

**DATASYNC = "DATA SPEED SYNC"**

* **D** ‚Äì Data transfer
* **A** ‚Äì Automated
* **T** ‚Äì Task-based
* **A** ‚Äì Agent (on-prem)
* **S** ‚Äì Secure (TLS)
* **Y** ‚Äì Your storage (S3/EFS/FSx)
* **N** ‚Äì Near real-time sync
* **C** ‚Äì Cloud migration

---

VPC Gateway Endpoint for S3 = creates a private route from your VPC to S3 over the AWS network (no NAT required, no public IPs required, no extra data processing charges)

App Runner ‚Üí ‚ÄúI just want my container to run, scale, and serve traffic ‚Äî no cluster headaches.‚Äù
EKS ‚Üí ‚ÄúI need full control, multiple microservices, custom networking, CI/CD integration, service mesh, and Kubernetes portability.‚Äù
+++++++++
Inbound endpoint = allows on-prem DNS servers to send queries into Route 53 Resolver (so VPC names can be resolved by on-prem resolvers by forwarding queries to the inbound endpoint).

Outbound endpoint = lets Route 53 Resolver forward queries out to your on-prem DNS servers (so resources in the VPC can resolve on-prem names by conditional forwarding through the outbound endpoint).
++

**CloudFormation StackSets** -extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation.

<img width="739" height="414" alt="image" src="https://github.com/user-attachments/assets/a91f1edf-bf47-4186-afc6-3e661a77a05f" />

**AWS CloudFormation template** - JSON or YAML-format, text-based file that describes all the AWS resources you need to deploy to run your application.

- blueprint for a stack

 **CloudFormation stacks** stack is a set of AWS resources that are created and managed as a single unit when AWS CloudFormation instantiates a template
    -  A stack cannot be used to deploy the same template across AWS accounts and regions

**AWS Resource Access Manager (AWS RAM)** AWS Resource Access Manager (AWS RAM) is a service that enables you to easily and securely share AWS resources with any AWS account or within your AWS Organization.


 **Amazon Machine Image (AMI)**

 provides the information required to launch an instance.

contains - One or more Amazon EBS snapshots, or, for instance-store-backed AMIs, a template for the root volume of the instance.

You can copy an AMI within or across AWS Regions using the AWS Management Console, the AWS Command Line Interface or SDKs, or the Amazon EC2 API, all of which support the CopyImage action. You can copy both Amazon EBS-backed AMIs and instance-store-backed AMIs. You can copy AMIs with encrypted snapshots and also change encryption status during the copy process.


<img width="1428" height="928" alt="image" src="https://github.com/user-attachments/assets/44ae91f4-1a23-4780-b5d1-7a343577ff2b" />

Think resource performance monitoring, events, and alerts; think **Amazon CloudWatch**.

Think account-specific activity and audit; think **AWS CloudTrail**.

Think resource-specific history, audit, and compliance; think **AWS Config**.

Global users + low-latency + UDP/TCP traffic = **AWS Global Accelerator**

Single-region + UDP/TCP traffic = **NLB**

HTTP/HTTPS traffic with Layer 7 features = **ALB**


**Amazon API Gateway HTTP APIs support native JWT authorizers**

Here‚Äôs a **crisp, exam-focused note** for **AWS Storage Gateway** that‚Äôs easy to remember:

---

# üìù **AWS Storage Gateway ‚Äì Exam Notes**

## ‚úÖ **What It Is**

* **Hybrid storage service** ‚Üí connects **on-premises apps** to **AWS cloud storage**
* Provides **low-latency local cache** + **virtually unlimited storage in S3**

---

## üõ† **Types of Gateways (Key for Exam)**

| **Gateway Type**   | **Use Case**                                     | **Key Points**                                                                                                                                                                   |
| ------------------ | ------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **File Gateway**   | Store & access **files** as objects in S3        | NFS/SMB access, integrates with S3 bucket                                                                                                                                        |
| **Volume Gateway** | Store **block storage volumes** in AWS           | Two modes: <br>üîπ **Cached Volumes:** Store primary data in S3, cache frequently accessed data locally <br>üîπ **Stored Volumes:** Store primary data locally, async backup to S3 |
| **Tape Gateway**   | Replace physical tape backups with virtual tapes | VTL interface, backups stored in S3/Glacier                                                                                                                                      |

---

## üß† **Exam Triggers**

* **Low-latency access + on-prem + cloud storage** ‚Üí Storage Gateway
* **Backup tapes to cloud** ‚Üí Tape Gateway
* **Extend local file server to S3** ‚Üí File Gateway
* **Block storage replication to S3** ‚Üí Volume Gateway (Cached/Stored)

---

## üìå **Memory Hook**

**FVT ‚Üí File, Volume, Tape**

* **F = Files to S3**
* **V = Volumes (Cached/Stored)**
* **T = Tape backup to S3/Glacier**

---


<img width="1116" height="663" alt="image" src="https://github.com/user-attachments/assets/ae5707cb-5b9f-49f4-9c70-05094ef0437e" />


<img width="1798" height="346" alt="image" src="https://github.com/user-attachments/assets/4b69a1ed-57c0-45bb-8b4f-432ea1f48322" />

<img width="1191" height="631" alt="image" src="https://github.com/user-attachments/assets/a87f7b23-de58-4296-bfdc-6d2b217fa60b" />

Got it ‚úÖ ‚Äî you want a **clear, exam-friendly breakdown** of **when to use which Amazon S3 storage class** ‚Äî with short, memorable tips.

Here‚Äôs the full guide:

---

# üìå **Amazon S3 Storage Classes ‚Äì Exam Cheat Sheet**

| **S3 Class**                                         | **When to Use (Ideal Scenario)**                                                                    | **Key Points / Keywords**                                                               |
| ---------------------------------------------------- | --------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------- |
| **S3 Standard**                                      | üîπ Frequently accessed data üîπ Mission-critical workloads üîπ Dynamic websites, content distribution | üåü **High durability (11 9s)**, low latency, multi-AZ, default choice                   |
| **S3 Standard-IA** (Infrequent Access)               | üîπ Data accessed **< once per month** üîπ Long-term backups, DR üîπ Not mission critical              | ‚úÖ Cheaper storage, **retrieval fee applies**, multi-AZ durability                       |
| **S3 One Zone-IA**                                   | üîπ Data you can easily **recreate** üîπ Secondary backups üîπ Logs, temp data                         | üí∞ \~20% cheaper than Standard-IA, **single AZ** (lower availability)                   |
| **S3 Intelligent-Tiering**                           | üîπ **Unknown / unpredictable access patterns** üîπ AI/ML datasets üîπ Data lakes                      | ü§ñ Auto-moves data between tiers (Frequent, Infrequent, Archive), no performance impact |
| **S3 Glacier Instant Retrieval**                     | üîπ Archival data that must be accessed **within ms** üîπ Medical images, media assets                | üí∏ Cheaper than IA, instant retrieval                                                   |
| **S3 Glacier Flexible Retrieval** (formerly Glacier) | üîπ Data rarely accessed üîπ Backup/Archive üîπ Retrieval within minutes‚Äìhours                         | üïí Bulk, Standard, Expedited retrieval options                                          |
| **S3 Glacier Deep Archive**                          | üîπ Long-term archive üîπ Regulatory compliance üîπ Retain for 7-10+ years                             | üïí Lowest cost storage, retrieval in **hours**                                          |
| **S3 Reduced Redundancy (Deprecated)**               | ‚ùå Do **not** use unless for very specific legacy cases                                              | AWS recommends Standard instead                                                         |

---

## üß† **Exam Tips & Keywords**

* **"Frequently accessed, low latency" ‚Üí S3 Standard**
* **"Infrequent but important, multi-AZ" ‚Üí S3 Standard-IA**
* **"Infrequent, can be lost or recreated, cost-sensitive" ‚Üí S3 One Zone-IA**
* **"Unknown access pattern, want automation" ‚Üí S3 Intelligent-Tiering**
* **"Archival but quick restore needed" ‚Üí Glacier Instant Retrieval**
* **"Archival, can wait minutes/hours" ‚Üí Glacier Flexible Retrieval**
* **"Cheapest, long-term compliance storage" ‚Üí Glacier Deep Archive**

---

## üìù **Memory Hook**

üîë **Hot ‚Üí Warm ‚Üí Cold ‚Üí Frozen**
(Standard ‚Üí IA ‚Üí Glacier ‚Üí Deep Archive)

Or use this mnemonic:
**"Some Important Objects Only Go Deep"**
(Standard, Intelligent-Tiering, One-Zone IA, Glacier, Deep Archive)

---


<img width="2538" height="1528" alt="image" src="https://github.com/user-attachments/assets/c7d7c8f5-514c-4fc0-8d7b-c0a1fde48dd9" />

You can't directly copy data from AWS Snowball Edge devices into Amazon S3 Glacier.

<img width="1058" height="413" alt="image" src="https://github.com/user-attachments/assets/3659ce58-d8f8-4bbf-8eea-2f2e7eea2023" />

<img width="2132" height="1200" alt="image" src="https://github.com/user-attachments/assets/92494fd6-5a8f-4c6c-bf05-5ca71d5a9adc" />

Perfect ‚úÖ ‚Äî this is another exam favorite. Let‚Äôs break down **Amazon EBS (Elastic Block Store) volume types**, their use cases, and when to choose each one.

---

# üìå **Amazon EBS Volume Types ‚Äì Exam Cheat Sheet**

EBS volumes are **block storage** for EC2 ‚Äî persistent, resizable, and AZ-specific.

---

## 1Ô∏è‚É£ **General Purpose SSD (gp2 / gp3)**

* **gp2** ‚Äì legacy, performance scales with size
* **gp3** ‚Äì latest generation, performance configurable independent of size
* **Performance:**

  * gp3 baseline: **3,000 IOPS + 125 MB/s throughput** (configurable up to 16,000 IOPS & 1,000 MB/s)
  * gp2: 3 IOPS per GB (max 16,000 IOPS)
* **Use When:**

  * ‚úÖ Boot volumes
  * ‚úÖ Small to medium DBs
  * ‚úÖ Dev/Test environments
  * ‚úÖ General-purpose workloads

**Exam Keyword:** "Most workloads" ‚Üí **gp3 (preferred)**

---

## 2Ô∏è‚É£ **Provisioned IOPS SSD (io1 / io2)**

* **Performance:**

  * io1/io2: Up to **64,000 IOPS** per volume
  * io2: Higher durability (99.999%)
* **Supports Multi-Attach** (attach to multiple EC2 instances at once, same AZ)
* **Use When:**

  * ‚úÖ Mission-critical databases (Oracle, SQL, SAP HANA, etc.)
  * ‚úÖ High-transaction, low-latency workloads
  * ‚úÖ Need predictable performance regardless of size

**Exam Keyword:** "High-performance DB" ‚Üí **io2**
**Exam Keyword:** "Multi-Attach required" ‚Üí **io1/io2 only**

---

## 3Ô∏è‚É£ **Throughput Optimized HDD (st1)**

* **Performance:**

  * Throughput-based (not IOPS)
  * Max throughput: \~500 MB/s
* **Use When:**

  * ‚úÖ Big data analytics
  * ‚úÖ Streaming workloads
  * ‚úÖ Log processing
  * ‚úÖ Large sequential workloads

**Exam Keyword:** "Big data, large sequential I/O" ‚Üí **st1**

---

## 4Ô∏è‚É£ **Cold HDD (sc1)**

* **Performance:**

  * Lowest cost
  * Max throughput: \~250 MB/s
* **Use When:**

  * ‚úÖ Infrequently accessed data
  * ‚úÖ Lowest-cost storage needed
  * ‚úÖ Archive data you still want online

**Exam Keyword:** "Lowest cost magnetic, rarely accessed" ‚Üí **sc1**

---

## üß† **Memory Hook**

Think of **EBS Volumes as Ladders of Performance vs Cost:**

üîº **Cost ‚Üë, Performance ‚Üë**
`sc1 (cold) ‚Üí st1 (throughput) ‚Üí gp3 (general) ‚Üí io2 (high IOPS)`

---

## ‚ö†Ô∏è **Common Pitfalls / Exam Gotchas**

* ‚ùå **gp2 performance scales with size** ‚Üí gp3 allows you to decouple size and performance.
* ‚ùå HDD volumes (st1, sc1) **cannot be boot volumes**.
* ‚ùå Multi-Attach is **only supported for io1/io2**, not gp3.
* ‚ùå Choose io2 for **business-critical DBs** (better durability than io1).

---

Here‚Äôs a **clear, exam-ready comparison table** for all EBS volume types ‚úÖ

---

# üìå **Amazon EBS Volume Types ‚Äì Comparison Table**

| **Type**                             | **IOPS (Max)**                       | **Throughput (Max)** | **Cost**           | **Boot Volume?** | **Best Use Case**                                            | **Exam Keyword**                    |
| ------------------------------------ | ------------------------------------ | -------------------- | ------------------ | ---------------- | ------------------------------------------------------------ | ----------------------------------- |
| **gp3 (General Purpose SSD)**        | 16,000                               | 1,000 MB/s           | üí≤üí≤ (balanced)    | ‚úÖ Yes            | General workloads, boot volumes, dev/test, small DBs         | "Default choice", "General-purpose" |
| **gp2 (Legacy)**                     | 16,000 (scales with size: 3 IOPS/GB) | 250 MB/s             | üí≤üí≤               | ‚úÖ Yes            | Same as gp3 (but gp3 preferred now)                          | "Older generation"                  |
| **io1 / io2 (Provisioned IOPS SSD)** | 64,000 (single volume)               | 1,000 MB/s           | üí≤üí≤üí≤üí≤ (highest) | ‚úÖ Yes            | Mission-critical DBs, OLTP apps, latency-sensitive workloads | "High IOPS", "Multi-Attach"         |
| **st1 (Throughput Optimized HDD)**   | \~500 IOPS (burst)                   | 500 MB/s             | üí≤ (low)           | ‚ùå No             | Big data, streaming, logs, large sequential I/O              | "Big data analytics", "Streaming"   |
| **sc1 (Cold HDD)**                   | \~250 IOPS (burst)                   | 250 MB/s             | üí≤ (lowest)        | ‚ùå No             | Archival, rarely accessed data, lowest cost storage          | "Lowest cost magnetic storage"      |

---

## üß† **Memory Hook**

* **gp3 = Go-to volume (Default)**
* **io2 = IOPS King (Databases)**
* **st1 = Streaming / Sequential**
* **sc1 = Cold Storage**

---

## üìù **Exam Quick Hits**

* ‚ùå **HDD volumes (st1, sc1) cannot be boot volumes.**
* ‚úÖ **io1/io2 only** support Multi-Attach.
* ‚úÖ Use **gp3** unless you have a clear need for higher IOPS/throughput or lower cost for sequential data.
* ‚úÖ Choose **io2** for **critical DBs** because of its higher durability (99.999%).

---



<img width="1536" height="1024" alt="image" src="https://github.com/user-attachments/assets/ddfc88f9-4b3a-471b-bc89-7721d73250e2" />

<img width="2658" height="1948" alt="image" src="https://github.com/user-attachments/assets/6265c1f5-8d02-4f69-8d3e-60fc601142a6" />

